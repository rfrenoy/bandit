%% Example of how-to-use bandit algorithms% In this example, we have to choose between K levers at each iteration.% Following the selection of a lever, we receive a reward. The reward is% drawn from gaussian distributions. Every lever is assigned a gaussian% distribution with unkown mean and standard deviation.%% Simulation parametersclear ; close all; clc     % InitializationK = 10;                    % Number of armsT = 10000;                 % Horizonreward_distrib = 'norm';   % Reward distributionverbose = false;           % Print ongoing results if set to truebandit_types = char('dummy','exp3','softmax','softmax\_dec','softmix','greedy\_first','greedy\_mix');gammas = [0.1,0.1,0.1,0.8,0.8,0.1,0.8]; % Gamma parameter for exploitation vs exploration tradeoffif size(bandit_types,1) ~= length(gammas)    fprintf('Error, please provide as many gamma parameters as there are algorithms\n');    fprintf('Nb gamma parameters: %d\tNb algorithms: %d',length(gammas),length(bandit_types));    return;end%% Select mean and std for reward distribution on every arms% Mean and standard deviation are randomly uniformly drawn from [0,1]mu = zeros(K,1);sigma = zeros(K,1);for k=1:K  mu(k) = random('unif',0,1);  sigma(k) = random('unif',0,1);  fprintf('Arm %d has mean %f and std %f\n',k,mu(k),sigma(k));endhold on; % Holding on to plot results from all algorithms in the same graphx = 1:1:T;%% Looping on every algorithmfor s=1:size(bandit_types,1)  % Instantiate algorithm with correct type  bandit_type = strtrim(bandit_types(s,:));  gamma = gammas(s);  switch bandit_type    case 'dummy'      bandit = dummy(K,gamma);    case 'exp3'      bandit = exp3(K,gamma);    case 'softmax'      bandit = softmax(K,gamma);    case 'softmax\_dec'      bandit = softmax_dec(K,gamma);    case 'softmix'      bandit = softmix(K,gamma);    case 'greedy\_first'      bandit = greedy_first(K,gamma,T);    case 'greedy\_mix'      bandit = greedy_mix(K,gamma);    otherwise      bandit = softmax(K,gamma);  end    %% Reset the simulation data  hist_rewards = [];  % Save rewards at every round  playcount = zeros(K,1);    %% Simulation for every algorithm  for t=1:T    [i,pb] = bandit.select();                         % Select arm based on the algorithm strategy    reward = random(reward_distrib,mu(i),sigma(i));   % Reward computed from corresponding distribution    bandit.update(i,reward);                          % Update weigths/probabilities    playcount(i) = playcount(i) + 1;        if verbose      fprintf('----- iteration %d -----\n',t);      for j=1:K        fprintf('Proba to select arm %d : %f\n',j,pb(j));      end      fprintf('Arm %d is selected with probability %f\n',i,pb(i));      fprintf('reward = %f\n',reward);      fprintf('\n\n');    end        hist_rewards = [hist_rewards,reward];  end  fprintf('\n\n----- Results for %s -----\n',bandit_type);  fprintf('Initial distribution per arm\n');  for k=1:K    fprintf('Arm %d has mean %f and std %f\tplayed %d times\n',k,mu(k),sigma(k),playcount(k));  end  fprintf('Refer to plot to see the pseudo-regret\n');  %% Recall that pseudo regret is T times mean of the best arm minus the cumulated reward  best_lever = max(mu);  cumul_hist = cumsum(hist_rewards);  pseudo_regret = x.*best_lever - cumul_hist;  plot(x,pseudo_regret);end%% Formatting plotstitle(['Pseudo regret with K=' num2str(K) ', T=' num2str(T) ', \gamma=' num2str(gamma)]);xlabel('Simulation iterations');ylabel('Pseudo-regret value');legend(bandit_types);hold off;   % Revealing every plots%% Conclusions% This simple simulation illustrates how to use the various bandit% algorithms implemented in this library. Recall that gamma values were% empirically specified, hence this simulation does not aim (at this time) at comparing% algorithms.